{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GD9gUQpaBxNa"
   },
   "source": [
    "# Train YOLOv5 on a Custom Dataset\n",
    "\n",
    "This tutorial is based on the [YOLOv5 repository](https://github.com/ultralytics/yolov5) by [Ultralytics](https://www.ultralytics.com/). This notebook shows how to train YOLOv5 on a custom dataset prepared in [Roboflow](https://roboflow.com/?ref=studiolab) using [AWS Studio Lab](https://studiolab.sagemaker.aws/). \n",
    "\n",
    "\n",
    "### Steps Covered in this Tutorial\n",
    "\n",
    "To train our detector we take the following steps:\n",
    "\n",
    "* Install YOLOv5 dependencies\n",
    "* Download custom YOLOv5 object detection data\n",
    "* Write our YOLOv5 Training configuration\n",
    "* Run YOLOv5 training\n",
    "* Evaluate YOLOv5 performance\n",
    "* Visualize YOLOv5 training data\n",
    "* Run YOLOv5 inference on test images\n",
    "* Export saved YOLOv5 weights for future inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Install YOLOv5 Dependencies\n",
    "\n",
    "## Connecting a GPU\n",
    "\n",
    "Training will go much faster if you use a GPU Runtime.\n",
    "\n",
    "From the Studio Lab homepage, select `GPU` as your `Compute type`.\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/LHpjx5x.png\" style=\"max-width: 450px;\"></div>\n",
    "\n",
    "To verify that you're running with a GPU, run the following command and ensure that it outputs your GPU stats (if you're running in a CPU runtime, this will give an error message instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "If there are no GPUs available, you will still be able to run this notebook, but training time will be greatly increased.\n",
    "\n",
    "## Cloning the YOLOv5 Repo\n",
    "\n",
    "Next, we'll pull down [the YOLOv5 repo](https://github.com/ultralytics/yolov5) from Github and install its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the working directory path for later use\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ie5uLDH4uzAp",
    "outputId": "d2e38392-e09e-4caf-e309-72b5354a5c9b"
   },
   "outputs": [],
   "source": [
    "# clone YOLOv5 repository\n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "%cd yolov5\n",
    "!git reset --hard fbe67e465375231474a2ad80a4389efc77ecff99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "484dd5f5-a33e-4e56-ac20-10dc71b36009"
   },
   "outputs": [],
   "source": [
    "# install dependencies as necessary\n",
    "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "from utils.downloads import attempt_download  # to download models/datasets\n",
    "\n",
    "# clear_output()\n",
    "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Roboflow\n",
    "\n",
    "We'll be using [Roboflow](https://roboflow.com/?ref=studiolab) to prepare and host our custom object detection dataset (and, optionally, to intelligently sample more images during inference to improve our dataset).\n",
    "\n",
    "The [`roboflow` pip package](https://blog.roboflow.com/pip-install-roboflow/) will load our dataset in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDIhrBF0sPaM",
    "tags": []
   },
   "source": [
    "# Preparing a Custom Dataset\n",
    "\n",
    "In order to train YOLOv5, we'll need a dataset which is composed of three parts:\n",
    "\n",
    "1. Images - ideally very similar to the ones our model will make predictions on.\n",
    "2. Annotations - special TXT files describing bounding boxes that our model will use to learn what it's looking for.\n",
    "3. A YAML File - contains configuration and metadata needed by YOLOv7 to understand our images and annotations.\n",
    "\n",
    "Our model will learn to fit the data in the training set, and evaluate its results against the validation set. At the end of the tutorial, we will try our model on the held-out test set to preview how it might perform in the wild when making predictions on images it has never seen before.\n",
    "\n",
    "We have two options for sourcing our dataset. We can create one from scratch (using our own images, and labeling it with our own annotations), or choose one that's been prepared and open sourced by someone else.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Option 1: Create a YOLOv5 Dataset with Your Own Images\n",
    "\n",
    "If you have our own images (and, optionally, annotations), click the three dots to expand the instructions for preparing your own dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "_**Note:** If you're running on Sagemaker Studio Lab you will probably want to stop your runtime while you create your dataset so you don't waste your quota. Depending on the size and complexity of your dataset, these steps may take a while to complete._\n",
    "\n",
    "### Step 1: [Sign up for a free Roboflow Account](https://app.roboflow.com/?ref=smsl-yolov5)\n",
    "\n",
    "[Roboflow](https://roboflow.com/?ref=studiolab) is an end-to-end computer vision platform. It helps you [create](https://docs.roboflow.com/quick-start?ref=studiolab), [understand](https://blog.roboflow.com/dataset-search/?ref=studiolab), and [use](https://docs.roboflow.com/exporting-data?ref=studiolab) image datasets to train and deploy custom models.\n",
    "\n",
    "Roboflow strives to be broadly interoperable and can import and export object detection datasets in [dozens of formats](https://roboflow.com/formats?ref=studiolab). They maintain [training notebooks](https://models.roboflow.com/?ref=studiolab) (like this one) for many state of the art computer vision models, and also offer [AutoML training](https://docs.roboflow.com/train?ref=studiolab) which can be useful for [prototyping](https://blog.roboflow.com/deploy-tab/?ref=studiolab), [model assisted labeling](https://roboflow.com/annotate?ref=studiolab), and even [deploying to a wide range of targets and edge devices](https://roboflow.com/deploy?ref=studiolab).\n",
    "\n",
    "In this tutorial, we will use Roboflow to annotate a custom dataset and export it for use with YOLOv7 in this notebook. But we encourage you to explore [its other features](https://roboflow.com/features?ref=studiolab) as well.\n",
    "\n",
    "### Step 2: Create a Public Workspace\n",
    "\n",
    "Roboflow offers a [generous free tier](https://roboflow.com/pricing?ref=studiolab) if your data can be shared publicly with others on [Roboflow Universe](https://universe.roboflow.com/?ref=studiolab). There are also paid plans available for private data.\n",
    "\n",
    "For this tutorial you'll need to create a Public workspace. Be sure to give it a good name; it will serve as your Universe username where you can showcase your work.\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/zfE5MZL.png\" style=\"max-width: 600px;\"></div>\n",
    "    \n",
    "### Step 3: Create a Project\n",
    "\n",
    "Then, create an `Object Detection` project (be sure to give it a descriptive name, and fill in the `What will your model predict?` section since they will make your project more understandable and be pulled in via the API later, and can serve as helpful metadata later for advanced use-cases like automated prompt engineering for zero-shot models).\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/O2xDyxQ.png\" style=\"max-width: 500px;\"></div>\n",
    "\n",
    "### Step 4: Upload your Images\n",
    "\n",
    "Next, drop image (or video) files into the UI and, optionally, drop existing annotations in [any supported format](https://roboflow.com/formats?ref=studiolab). Alternatively you can use the [Upload API](https://docs.roboflow.com/adding-data/upload-api?ref=studiolab) or [load images from an S3 bucket](https://blog.roboflow.com/how-to-use-s3-computer-vision-pipeline/?ref=studiolab).\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/hWrhtNj.png\" style=\"max-width: 600px;\"></div>\n",
    "\n",
    "Then click `Finish Uploading` to add the images to your Roboflow project.\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/9hUh9j5.png\" style=\"max-width: 220px;\"></div>\n",
    "\n",
    "_**Note:** To get good, generalizable results you will need lots of images covering a wide variety of situations and edge cases. Exactly how many images you need [depends on a wide variety of factors](https://blog.roboflow.com/images-train-model/?ref=studiolab), but we recommend starting out with at least 200 for most use-cases. If you need more images, try sourcing from open source datasets on [Roboflow Universe](https://universe.roboflow.com/?ref=studiolab) with images similar to yours._\n",
    "\n",
    "### Step 5: Annotate\n",
    "\n",
    "_**Note:** If you imported annotations from another labeling tool or open source dataset, you can skip this step._\n",
    "\n",
    "Now, we'll use [Roboflow Annotate](https://roboflow.com/annotate?ref=studiolab) to create annotations that will teach our model what we're trying to detect in our images. Since your model will learn to mimic your annotations, it's important that you give some thought to how you label your images ahead of time. We've compiled a list of [best practices to consider when labeling images](https://blog.roboflow.com/tips-for-how-to-label-images/?ref=studiolab).\n",
    "\n",
    "You can either choose to annotate the images yourself, or invite a friend to your workspace to help out for double the fun.\n",
    "\n",
    "<video loop autoplay controls src=\"https://i.imgur.com/AuDAPs4.mp4\">Annotate Images</video>\n",
    "\n",
    "\n",
    "### Step 6: Add to Dataset\n",
    "\n",
    "Once you have annotated 200 images or more, you're ready to add them to your dataset. At this point you'll choose how to split your images into train, valid, and test sets. We usually recommend keeping the 70/20/10 ratio unless you have more than a few thousand images, in which case you might want to add a higher percentage to your test set.\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/XUVE6uq.png\" style=\"max-width: 400px;\"></div>\n",
    "\n",
    "If you're an advanced user, you can also choose to craft these sets by hand to verify that your model is going to generalize well.\n",
    "\n",
    "### Step 7: View the Health Check (Optional)\n",
    "\n",
    "Roboflow includes a [dataset health check](https://docs.roboflow.com/dataset-health-check?ref=studiolab) which gives information about your class balance, box distribution, and image sizes. Checking this can help you choose good preprocessing and augmentation steps while generating a version of your dataset which can improve your model's performance.\n",
    "\n",
    "For example, if you discover your objects are all clustered around the center of the image, you may want to [apply a static crop](https://docs.roboflow.com/image-transformations/image-preprocessing#static-crop) so your model can focus only on the important parts of the image. Or if your images have giant dimensions compared to your objects of interes, [you might want to apply tiling](https://blog.roboflow.com/detect-small-objects/?ref=studiolab).\n",
    "\n",
    "### Step 8: Generate a Version\n",
    "\n",
    "Roboflow helps you version control your datasets so that you can get repeatable results & track changes and performance over time. It also lets you preprocess and augment your images which can speed up your training time and improve your model's results.\n",
    "\n",
    "Choose your preprocessing steps. For YOLOv5, we recommend [Auto-Orient](https://blog.roboflow.com/exif-auto-orientation/?ref=studiolab) and Resize (Stretch to 640x640), which is YOLOv5's default input size. You may also want to [enable Tiling](https://blog.roboflow.com/edge-tiling-during-inference/?ref=studiolab) if your objects are very small compared to the size of your images (if you're not sure, try training a model first without, and if you get poor results you can try again).\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/SZojEwP.png\" style=\"max-width: 400px;\"></div>\n",
    "\n",
    "Next, choose your desired augmentations. YOLOv5 does online augmentations during training automatically, so if you have more than 500 annotated images in your dataset you can usually skip this step. But for smaller datasets we've seen good results adding some basic augmentations which mimic things your model may see in the while. We recommend **not** adding Cutout or Mosaic for YOLOv5 as these are already applied during training and applying them twice produces poor results.\n",
    "\n",
    "Finally, click `Generate` to lock in your choices and render your images.\n",
    "\n",
    "### Step 9: Export for YOLOv5\n",
    "\n",
    "Once you've generated a dataset version, click `Export` and select the `YOLO v5 PyTorch` format and the `show download code` option.\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/luQC9Rg.png\" style=\"max-width: 500px;\"></div>\n",
    "\n",
    "This will convert your dataset to the proper format and make it available for use in this notebook.\n",
    "\n",
    "### Step 9: Copy Your Snippet\n",
    "\n",
    "Now you're all set, simply copy and paste the download snippet from the `Jupyter` tab into the code cell below and you're ready to train your custom YOLOv5 model.\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/hZJYgdy.png\" style=\"max-width: 500px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Option 2: Use an Open Source Dataset\n",
    "\n",
    "[Roboflow Universe](https://universe.roboflow.com/?ref=studiolab) is the world's largest repository of open source datasets. There are [over 100,000 datasets to choose from](https://blog.roboflow.com/computer-vision-datasets-and-apis/?ref=studiolab) for [a plethora of use-cases](https://universe.roboflow.com/browse?ref=studiolab).\n",
    "\n",
    "If you'd like to start from an open source dataset, click the three dots to expand the instructions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Find a Dataset\n",
    "\n",
    "With over 100,000 open source datasets (and over 10,000 of those with a pre-trained model you can try in your browser), it's a near-certainty that you can find a starting point for nearly any computer vision problem on Roboflow Universe.\n",
    "\n",
    "If you're looking for inspiration, browse [some of the most recently updated projects](https://universe.roboflow.com/search?q=object%20detection%20images%3E%3D100%20images%3C%3D1000%20has%3Amodel&ref=studiolab) or check out some of our [curated collections of datasets](https://universe.roboflow.com/browse?ref=studiolab).\n",
    "\n",
    "### Step 2: Explore its Contents (Optional)\n",
    "\n",
    "Once you've found something that looks interesting, you can [explore the images using Dataset Search](https://blog.roboflow.com/dataset-search/?ref=studiolab) to make sure its images cover all the cases you're looking for.\n",
    "\n",
    "If nothing is quite right, you may want to combine specific images from multiple datasets into a new custom project instead. For example, [the Microsoft COCO Dataset](https://blog.roboflow.com/coco-dataset/?ref=studiolab) doesn't have a class for graffiti, but [it has many images containing graffiti](https://universe.roboflow.com/jacob-solawetz/microsoft-coco/browse?queryText=graffiti&pageSize=50&startingIndex=0&browseQuery=true&ref=studiolab) that you could use to bootstrap a new dataset along with some [other open source graffiti datasets](https://universe.roboflow.com/search?q=graffiti%20object%20detection%20images%3E100&ref=studiolab).\n",
    "\n",
    "### Step 3: Try a Pre-Trained Model in Your Browser (Optional)\n",
    "\n",
    "Since many other users of Roboflow Universe have already trained a model on their datasets, you can oftentimes get a sneak preview of how your YOLOv7 model might perform by trying it out in your webcam or on some sample images using [the project's Model tab](https://blog.roboflow.com/deploy-tab/?ref=studiolab).\n",
    "\n",
    "<video loop autoplay controls src=\"https://i.imgur.com/yWwbFIs.mp4\">Try a Pre-Trained Model in Your Browser</video>\n",
    "\n",
    "Even though these pre-trained models don't use YOLOv7, oftentimes the limiting factor of prediction quality is in the data, not the model. So they can give you some preliminary intelligence on how well your YOLOv5 model might perform if you train it on that dataset.\n",
    "\n",
    "### Step 4: Choose a Dataset Version\n",
    "\n",
    "Many datasets on Roboflow Universe have multiple versions that were generated with different settings (eg different image sizes, and augmentation steps) and, potentially, with different sets of images as the maintainer [added more through active learning](https://blog.roboflow.com/computer-vision-active-learning-tips/?ref=studiolab) over time. \n",
    "\n",
    "_**PROTIP:** If multiple versions of the dataset have trained models on Roboflow Universe, you may want to choose the one that achieved the highest mean average precision as it can be a signal of higher._\n",
    "\n",
    "### Step 5: Get Your Download Snippet\n",
    "\n",
    "When you've found a dataset version that suits your needs, click `Download` and select the `YOLO v5 PyTorch` format. This will give you a code snippet that will load your dataset into this notebook when you paste it into the code cell below.\n",
    "\n",
    "<div><img src=\"https://i.imgur.com/hZJYgdy.png\" style=\"max-width: 500px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset in YOLOv5 Format\n",
    "\n",
    "Now that we have a dataset, we'll download it into our notebook environment in the right format. Use the `YOLO v5 PyTorch` export option in Roboflow.\n",
    "\n",
    "The YOLOv5 requires YOLO TXT annotations, a custom YAML file, and organized directories. Roboflow creates and hosts this for us and helps download and configure it for our model to consume.\n",
    "\n",
    "**Copy and paste your code snippet from Roboflow into the code cell below.** The snippet contains a reference to the dataset and an API Key that will authorize you to access your private data from Roboflow (if applicable) and perform (optional) advanced options later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ug_PhK1oqwQA",
    "outputId": "b417162b-c116-416c-88e8-96b2563d1ee1"
   },
   "outputs": [],
   "source": [
    "%cd {HOME}/yolov5 \n",
    "#after following the link above, recieve python code with these fields filled in\n",
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"YOUR_ROBOFLOW_API_KEY\")\n",
    "project = rf.workspace(\"joseph-nelson\").project(\"bccd\")\n",
    "dataset = project.version(4).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZ3DmmGQztJj",
    "outputId": "895f8313-8441-4c2d-9c87-01cc89f22acb"
   },
   "outputs": [],
   "source": [
    "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
    "%cat {dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwJx-2NHsYxT"
   },
   "source": [
    "# Define Model Configuration and Architecture\n",
    "\n",
    "We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n",
    "\n",
    "You do not need to edit these cells, but you may."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOPn9wjOAwwK"
   },
   "outputs": [],
   "source": [
    "# define number of classes based on YAML\n",
    "import yaml\n",
    "with open(dataset.location + \"/data.yaml\", 'r') as stream:\n",
    "    num_classes = str(yaml.safe_load(stream)['nc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Rvt5wilnDyX",
    "outputId": "8b8386bc-0e05-49a8-c1e0-48a836a0b399"
   },
   "outputs": [],
   "source": [
    "#this is the model configuration we will use for our tutorial \n",
    "%cat {HOME}/yolov5/models/yolov5s.yaml #COLAB_EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t14hhyqdmw6O"
   },
   "outputs": [],
   "source": [
    "#customize iPython writefile so we can write variables\n",
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDxebz13RdRA"
   },
   "outputs": [],
   "source": [
    "%%writetemplate {HOME}/yolov5/models/custom_yolov5s.yaml\n",
    "\n",
    "# parameters\n",
    "nc: {num_classes}  # number of classes\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.50  # layer channel multiple\n",
    "\n",
    "# anchors\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# YOLOv5 backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
    "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
    "   [-1, 3, BottleneckCSP, [128]],\n",
    "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
    "   [-1, 9, BottleneckCSP, [256]],\n",
    "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
    "   [-1, 9, BottleneckCSP, [512]],\n",
    "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
    "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
    "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
    "  ]\n",
    "\n",
    "# YOLOv5 head\n",
    "head:\n",
    "  [[-1, 1, Conv, [512, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
    "\n",
    "   [-1, 1, Conv, [256, 1, 1]],\n",
    "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
    "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
    "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
    "\n",
    "   [-1, 1, Conv, [256, 3, 2]],\n",
    "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
    "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
    "\n",
    "   [-1, 1, Conv, [512, 3, 2]],\n",
    "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
    "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
    "\n",
    "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUOiNLtMP5aG"
   },
   "source": [
    "# Train Custom YOLOv5 Detector\n",
    "\n",
    "### Next, we'll fire off training!\n",
    "\n",
    "\n",
    "Here, we are able to pass a number of arguments:\n",
    "- **img:** define input image size\n",
    "- **batch:** determine batch size\n",
    "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
    "- **data:** set the path to our yaml file\n",
    "- **cfg:** specify our model configuration\n",
    "- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https://drive.google.com/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n",
    "- **name:** result names\n",
    "- **nosave:** only save the final checkpoint\n",
    "- **cache:** cache images for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NcFxRcFdJ_O",
    "outputId": "84614320-0d9c-40c2-dbef-a2c121ab667b"
   },
   "outputs": [],
   "source": [
    "# train yolov5s on custom data for 100 epochs\n",
    "# time its performance\n",
    "#%%time  #COLAB_EDIT\n",
    "%cd {HOME}/yolov5/ \n",
    "!python train.py --img 416 --batch 16 --epochs 100 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights '' --name yolov5s_results  --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJVs_4zEeVbF"
   },
   "source": [
    "# Evaluate Custom YOLOv5 Detector Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KN5ghjE6ZWh"
   },
   "source": [
    "Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov5s_results`. (If given no name, it defaults to `results.txt`.) The results file is plotted as a png after training completes.\n",
    "\n",
    "Note from Glenn: Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "C60XAsyv6OPe",
    "outputId": "d9ca801f-ef76-4b33-bc84-51dcc8d68a85"
   },
   "outputs": [],
   "source": [
    "from utils.plots import plot_results  # plot results.txt as results.png\n",
    "Image(filename=HOME+'/yolov5/runs/train/yolov5s_results/results.png', width=1000)  # view results.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLI1JmHU7B0l"
   },
   "source": [
    "### Curious? Visualize Our Validation Data with Labels\n",
    "\n",
    "After training starts, view `valid*.jpg` images to see validation images, labels and augmentation effects.\n",
    "\n",
    "Note a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Glenn Jocher and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "PF9MLHDb7tB6",
    "outputId": "be844e31-99a3-4aae-c968-235a52365ac0"
   },
   "outputs": [],
   "source": [
    "# first, display our ground truth data\n",
    "\n",
    "print(\"GROUND TRUTH VALIDATION DATA:\")\n",
    "Image(filename=HOME+'/yolov5/runs/train/yolov5s_results/val_batch0_labels.jpg', width=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "id": "W40tI99_7BcH",
    "outputId": "cccf6f3d-131b-48c9-b535-3dca782b3aaa"
   },
   "outputs": [],
   "source": [
    "# print out an augmented training example\n",
    "print(\"GROUND TRUTH AUGMENTED TRAINING DATA:\")\n",
    "Image(filename=HOME+'/yolov5/runs/train/yolov5s_results/train_batch0.jpg', width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3qM6T0W53gh"
   },
   "source": [
    "#Run Inference  With Trained Weights\n",
    "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIEwt5YLeQ7P",
    "outputId": "d042529c-2d05-49e1-f53c-a8aac20eaab7"
   },
   "outputs": [],
   "source": [
    "# trained weights are saved by default in our weights folder\n",
    "%ls runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4SyOWS80qR32",
    "outputId": "53ece1df-82ed-4db9-eb7c-7de938ee493c"
   },
   "outputs": [],
   "source": [
    "%ls runs/train/yolov5s_results/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nmZZnWOgJ2S",
    "outputId": "9d56f834-e5f3-43dc-b935-a4c5827e2c36"
   },
   "outputs": [],
   "source": [
    "# when we ran this, we saw .007 second inference time. That is 140 FPS on a TESLA P100!\n",
    "# use the best weights!\n",
    "%cd {HOME}/yolov5\n",
    "!python detect.py --weights runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.4 --source {dataset.location}/test/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odKEqYtTgbRc"
   },
   "outputs": [],
   "source": [
    "#display inference on ALL test images\n",
    "#this looks much better with longer training above\n",
    "\n",
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "for imageName in glob.glob(HOME+'/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
    "    display(Image(filename=imageName))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
